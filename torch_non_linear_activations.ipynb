{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgInyse5SQsZnvGgIynPpj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanubv/torch/blob/main/torch_non_linear_activations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgZwdaFSbTMT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ELU- Exponential Linear Unit\n",
        "#Syntax: torch.nn.ELU(alpha = 1.0, inplace= bool)\n",
        "#Example:\n",
        "m = nn.ELU()\n",
        "inputs = torch.randn(2)\n",
        "print(inputs)\n",
        "outputs = m(inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCz6x_0ibb08",
        "outputId": "f389c473-4604-44d3-d1cc-b9bb4f55767d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.9239, -1.2277])\n",
            "tensor([-0.8540, -0.7070])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hardshrink activation\n",
        "#Syntax: torch.nn.Hardshrink(lamda=0.5)\n",
        "#example:\n",
        "m = nn.Hardshrink()\n",
        "inputs = torch.randn(2)\n",
        "print(inputs)\n",
        "outputs = m(inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-2wl4Nmdmkr",
        "outputId": "7a238ad0-0ed6-4dde-ad05-64836d2ef717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.2106, 0.9303])\n",
            "tensor([0.0000, 0.9303])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ZBE1LCukeH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hardsigmoid Activation\n",
        "#Syntax: torch.nn.Hardsigmoid(inplace = False)\n",
        "#Example:\n",
        "m = nn.Hardsigmoid()\n",
        "inputs = torch.randn(2)\n",
        "print(inputs)\n",
        "outputs = m(inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWRaNB_GfTdq",
        "outputId": "b66d2209-1322-412a-ae2d-b37678ae8a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1234, 0.5599])\n",
            "tensor([0.5206, 0.5933])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#HardTanH Activation\n",
        "#syntax: torch.nn.Hardtanh(min_values = -1, max_value = 1, inplace = bool)\n",
        "#Example:\n",
        "m = nn.Hardtanh(-2, 2)\n",
        "inputs = torch.randn(2, 10)\n",
        "print(inputs)\n",
        "outputs = m(inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s5vVSfsfTzf",
        "outputId": "7f02eb9a-0a5a-4c35-f4e8-71a4b7e2364a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.3187,  0.2429, -0.2304, -1.0415,  0.4975, -0.2751, -0.9131,  0.1001,\n",
            "          1.0210, -0.1435],\n",
            "        [ 0.0196, -0.2500, -0.3773,  0.3193,  0.7646,  2.0715,  0.0671, -0.3972,\n",
            "          1.1680, -1.5815]])\n",
            "tensor([[-0.3187,  0.2429, -0.2304, -1.0415,  0.4975, -0.2751, -0.9131,  0.1001,\n",
            "          1.0210, -0.1435],\n",
            "        [ 0.0196, -0.2500, -0.3773,  0.3193,  0.7646,  2.0000,  0.0671, -0.3972,\n",
            "          1.1680, -1.5815]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Leaky ReLU Activation\n",
        "\"\"\"\n",
        "Mathematical representation of leaky relu\n",
        "LeakyReLU(x) = max(0, x)+negative_slope*min(0,x)\n",
        "Synatx: torch.nn.LeakyReLU(negative_slope = 0.01, inplace = bool)\n",
        "\"\"\"\n",
        "#Example:\n",
        "m = nn.LeakyReLU(0.01)\n",
        "inputs = torch.randn(2)\n",
        "print(inputs)\n",
        "outputs = m(inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H64hsDyJt8eC",
        "outputId": "37a8e8d5-2416-451f-c264-a3ead8193c15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.0726, -1.1147])\n",
            "tensor([-0.0007, -0.0111])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LogSimoid activation function\n",
        "\"\"\"\n",
        "The Mathematical representation for logsigmoid activation function is\n",
        "LogSigmoid(x) = 1/1+exp(-1)\n",
        "Syntax: torch.nn.LogSigmoid(*args, **kwargs)\n",
        "\"\"\"\n",
        "#Example:\n",
        "m = nn.LogSigmoid()\n",
        "inputs = torch.randn(2)\n",
        "print(inputs)\n",
        "outputs = m(inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwTuVqYruZSM",
        "outputId": "58cf5c27-1bae-43c8-f211-98081b4f1bf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.5504, -1.3586])\n",
            "tensor([-0.4553, -1.5874])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Parameterised ReLU activation function\n",
        "\"\"\"\n",
        "The mathematical representation of parameterised relu can be shown as below\n",
        "PReLU(x) = max(0, x)+a*min(0, x)\n",
        "syntax = torch.nn.PReLU()\n",
        "\"\"\"\n",
        "#Example:\n",
        "m = nn.PReLU()\n",
        "inputs = torch.randn(2)\n",
        "print(inputs)\n",
        "outputs = m(inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEIDGAPyzQyF",
        "outputId": "b11b2504-6a8b-4790-fe1b-bc79f98d0dc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-2.8286,  0.0981])\n",
            "tensor([-0.7072,  0.0981], grad_fn=<PreluKernelBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ReLU Activation fucntion\n",
        "\"\"\"\n",
        "Applies ReLU activation function over all the inputs elementwise\n",
        "The mathematical representation of ReLU can be shown as below\n",
        "ReLu(x) = max(0, x)\n",
        "Syntax: torch.nn.ReLU(inplace = False)\n",
        "\"\"\"\n",
        "#Example:\n",
        "m = nn.ReLU()\n",
        "inputs = torch.randn(2)\n",
        "print(inputs)\n",
        "outputs = m(inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrhB3MTHzQ7E",
        "outputId": "4d836af8-ad85-47aa-8a16-ed3bc06ba75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.2027, -0.3669])\n",
            "tensor([0.2027, 0.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sigmoid Activation Function\n",
        "\"\"\"\n",
        "The mathematical representation of sigmoidal fucntion can be shown as below.\n",
        "Sigmoid(x) = 1/(1+exp(-x))\n",
        "Syntax:torch.nn.Sigmoid(*args, **kwargs)\n",
        "\"\"\"\n",
        "#example:\n",
        "m = nn.Sigmoid()\n",
        "inputs = torch.randn(2)\n",
        "print(inputs)\n",
        "outputs = m(inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "603pxHJIzQ-X",
        "outputId": "09cb6532-66cb-48ba-f3fe-37cd284c7e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.2966,  0.3475])\n",
            "tensor([0.4264, 0.5860])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TanH Activation Function\n",
        "\"\"\"\n",
        "Applies hyperbolic tangent fucntion elementwise on input\n",
        "The mathematical representation of TanH function can be shown as\n",
        "Tanh(x) = (exp(x)-exp(-x))/(exp(x)+exp(-x))\n",
        "syntax: torch.nn.Tanh(*args, **kwargs)\n",
        "\"\"\"\n",
        "#Syntax:\n",
        "m = nn.Tanh()\n",
        "inputs = torch.randn(2)\n",
        "print(inputs)\n",
        "ouputs = m(inputs)\n",
        "print(outputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUnxhaEb5K-p",
        "outputId": "99c0ea4f-5564-4ea7-ce35-de450f68251a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.6292,  0.8813])\n",
            "tensor([0.4264, 0.5860])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SoftMin activation function\n",
        "\"\"\"\n",
        "Applies Softmin activation to n-dimensional input tensor and rescaling them so that the output of n-dimensional lies with in the range of (0, 1) and sums to 1\n",
        "The mathematical representation fo Softmin activation function can be shown as below\n",
        "Softmin(x) = exp(-x)/sum(exp(-x))\n",
        "syntax: torch.nn.Softmin(dim = none)\n",
        "\"\"\"\n",
        "#Example\n",
        "m = nn.Softmin(dim=1)\n",
        "inputs = torch.randn(2,2)\n",
        "print(inputs)\n",
        "ouputs = m(inputs)\n",
        "print(outputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi_e1FIW6GHB",
        "outputId": "5b3dbb64-789b-45ce-d0e0-c936768724d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.6383, -0.0618],\n",
            "        [-0.8419,  0.9201]])\n",
            "tensor([0.4264, 0.5860])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SoftMax activation function\n",
        "\"\"\"\n",
        "Applies Softmax activation to n-dimensional input tensor and rescaling them so that the output of n-dimensional lies with in the range of (0, 1) and sums to 1\n",
        "The mathematical representation fo Softmax activation function can be shown as below\n",
        "Softmax(x) = exp(x)/sum(exp(x))\n",
        "syntax: torch.nn.Softmax(dim = none)\n",
        "\"\"\"\n",
        "#Example\n",
        "m = nn.Softmax(dim= 1)\n",
        "inputs = torch.randn(2,2)\n",
        "print(inputs)\n",
        "outputs = m(inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5WAnOlL6GJd",
        "outputId": "1f0fc4ae-6708-434b-ad12-8ee946ce4d49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.8739,  0.0778],\n",
            "        [-0.6867, -0.6733]])\n",
            "tensor([[0.6891, 0.3109],\n",
            "        [0.4967, 0.5033]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Softmax2D\n",
        "\"\"\"\n",
        "Softmax2d applies over features in each loacation if given input image is having channels, height and width then softmax function will be applied on each loaction\n",
        "(Channels, height, width)\n",
        "Syntax:\n",
        "torch.nn.Softmax2d(*args, **kwargs)\n",
        "\"\"\"\n",
        "#Example:\n",
        "m = nn.Softmax2d()\n",
        "inputs = torch.randn(2, 3, 12)\n",
        "print(inputs)\n",
        "outputs = m(inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3MWa1Dr6GLz",
        "outputId": "6d15d0ae-1017-42ee-db58-958cb1cc242b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.7557, -0.4461,  1.1434,  0.0063, -0.4778,  1.6566, -0.7878,\n",
            "          -1.1018, -0.5400,  0.8439, -0.4263,  0.8545],\n",
            "         [-1.2879,  0.5243,  1.1545,  0.3117, -0.7260,  0.3276, -0.2623,\n",
            "          -1.2575, -0.0736,  0.2189, -0.2590,  0.8629],\n",
            "         [-1.8271,  0.1600,  0.5989,  0.7941, -0.3992,  0.8186, -1.4618,\n",
            "           0.0214,  1.5286,  0.8333, -0.9580,  2.1688]],\n",
            "\n",
            "        [[-0.9112,  0.2107,  0.9831,  0.3755, -0.2410, -0.0062,  0.0242,\n",
            "           2.1633,  0.1958, -0.6860,  1.0918,  0.8691],\n",
            "         [-0.1948,  0.5144, -0.6221,  0.9815, -1.2554,  1.8999, -0.1630,\n",
            "           0.5094, -1.2041,  1.1137,  0.4256, -1.2007],\n",
            "         [-0.2924, -0.3040,  0.7996, -1.2166, -2.2271,  1.2668,  1.1329,\n",
            "           1.2308,  1.4826, -0.6486, -0.1821, -0.1981]]])\n",
            "tensor([[[0.8412, 0.3414, 0.5400, 0.4087, 0.4411, 0.8406, 0.3075, 0.0368,\n",
            "          0.3239, 0.8220, 0.1797, 0.4963],\n",
            "         [0.2510, 0.5025, 0.8553, 0.3385, 0.6294, 0.1719, 0.4752, 0.1459,\n",
            "          0.7559, 0.2901, 0.3352, 0.8873],\n",
            "         [0.1773, 0.6140, 0.4500, 0.8819, 0.8615, 0.3898, 0.0695, 0.2298,\n",
            "          0.5115, 0.8149, 0.3152, 0.9143]],\n",
            "\n",
            "        [[0.1588, 0.6586, 0.4600, 0.5913, 0.5589, 0.1594, 0.6925, 0.9632,\n",
            "          0.6761, 0.1780, 0.8203, 0.5037],\n",
            "         [0.7490, 0.4975, 0.1447, 0.6615, 0.3706, 0.8281, 0.5248, 0.8541,\n",
            "          0.2441, 0.7099, 0.6648, 0.1127],\n",
            "         [0.8227, 0.3860, 0.5500, 0.1181, 0.1385, 0.6102, 0.9305, 0.7702,\n",
            "          0.4885, 0.1851, 0.6848, 0.0857]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KBpO97zv5LRA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}